{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxi74u/llm_diacritization_finetuning/blob/main/notebooks/finetuning_with_QLoRA\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb55eb9c",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-05-11T14:07:08.996732Z",
          "iopub.status.busy": "2025-05-11T14:07:08.996469Z",
          "iopub.status.idle": "2025-05-11T14:08:27.877931Z",
          "shell.execute_reply": "2025-05-11T14:08:27.877148Z"
        },
        "id": "fb55eb9c",
        "outputId": "9a9651d7-4c4f-4ee5-97fa-486290483dee",
        "papermill": {
          "duration": 78.888294,
          "end_time": "2025-05-11T14:08:27.879314",
          "exception": false,
          "start_time": "2025-05-11T14:07:08.991020",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\r\n",
            "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
            "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
            "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece protobuf pyarabic torch huggingface_hub wandb # Optional for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6dd1147",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:27.926477Z",
          "iopub.status.busy": "2025-05-11T14:08:27.926216Z",
          "iopub.status.idle": "2025-05-11T14:08:57.410565Z",
          "shell.execute_reply": "2025-05-11T14:08:57.409761Z"
        },
        "id": "f6dd1147",
        "outputId": "b377657f-b7b0-443d-be35-4bde6d4125db",
        "papermill": {
          "duration": 29.509765,
          "end_time": "2025-05-11T14:08:57.412071",
          "exception": false,
          "start_time": "2025-05-11T14:08:27.902306",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-11 14:08:41.943928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746972522.124948      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746972522.177843      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from wandb import wandb\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "import pyarabic.araby\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c6b934",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:57.459362Z",
          "iopub.status.busy": "2025-05-11T14:08:57.458830Z",
          "iopub.status.idle": "2025-05-11T14:08:57.983350Z",
          "shell.execute_reply": "2025-05-11T14:08:57.982754Z"
        },
        "id": "93c6b934",
        "outputId": "866c0b16-5479-4f20-b536-90ab602a04bf",
        "papermill": {
          "duration": 0.549219,
          "end_time": "2025-05-11T14:08:57.984753",
          "exception": false,
          "start_time": "2025-05-11T14:08:57.435534",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No module named 'kaggle_secrets'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "  secret_value_1 = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y7EjjPi-PsTd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7EjjPi-PsTd",
        "outputId": "5268a1c2-dccc-42cf-e769-6d2dafedb942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded secrets from environment variables (local).\n",
            "✅ Loaded secrets via Colab input.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "secret_value_0 = None\n",
        "secret_value_1 = None\n",
        "\n",
        "try:\n",
        "    import os\n",
        "    secret_value_0 = os.environ.get(\"HF_TOKEN\")\n",
        "    secret_value_1 = os.environ.get(\"WANDB_TOKEN\")\n",
        "    print(\"✅ Loaded secrets from environment variables (local).\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load secrets. Exception: {type(e).__name__}: {e}\")\n",
        "\n",
        "# Try importing kaggle_secrets for Kaggle\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "    secret_value_1 = user_secrets.get_secret(\"WANDB_TOKEN\")\n",
        "    print(\"✅ Loaded secrets from Kaggle.\")\n",
        "except ImportError:\n",
        "    pass  # Proceed to other methods if not Kaggle\n",
        "\n",
        "# Try getting secrets from Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    secret_value_0 = userdata.get(\"HF_TOKEN\")\n",
        "    secret_value_1 = userdata.get('WANDB_TOKEN')\n",
        "    print(\"✅ Loaded secrets via Colab input.\")\n",
        "except ImportError:\n",
        "    pass  # Proceed to other methods if not Colab\n",
        "\n",
        "# Try getting secrets from environment variables for local use\n",
        "\n",
        "\n",
        "# Check if secrets are loaded successfully\n",
        "if not secret_value_0 or not secret_value_1:\n",
        "    print(\"❌ Some secrets are missing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4374d713",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:58.036729Z",
          "iopub.status.busy": "2025-05-11T14:08:58.036492Z",
          "iopub.status.idle": "2025-05-11T14:08:58.479851Z",
          "shell.execute_reply": "2025-05-11T14:08:58.479242Z"
        },
        "id": "4374d713",
        "outputId": "aa35b03b-d3cb-463f-afa0-cf7791933033",
        "papermill": {
          "duration": 0.4677,
          "end_time": "2025-05-11T14:08:58.480960",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.013260",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbishertello\u001b[0m (\u001b[33mbishertello-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Your Hugging Face token as a string\n",
        "token = secret_value_0\n",
        "\n",
        "# Save the token so that other Hugging Face tools can use it\n",
        "HfFolder.save_token(token)\n",
        "\n",
        "# (Optional) If you want to use the API directly\n",
        "api = HfApi(token=token)\n",
        "\n",
        "# Your W&B API key\n",
        "api_key = secret_value_1\n",
        "\n",
        "\n",
        "# Log in programmatically\n",
        "wandb.login(key=api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca236607",
      "metadata": {
        "id": "ca236607",
        "papermill": {
          "duration": 0.022311,
          "end_time": "2025-05-11T14:08:58.526524",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.504213",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df87a79",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:58.572533Z",
          "iopub.status.busy": "2025-05-11T14:08:58.572237Z",
          "iopub.status.idle": "2025-05-11T14:08:58.576640Z",
          "shell.execute_reply": "2025-05-11T14:08:58.576132Z"
        },
        "id": "0df87a79",
        "papermill": {
          "duration": 0.029008,
          "end_time": "2025-05-11T14:08:58.577741",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.548733",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Model ID\n",
        "base_model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "# Dataset ID\n",
        "dataset_id = \"Bisher/fadel-diacritization\"\n",
        "# New model ID for saving adapters\n",
        "new_model_id = f\"{base_model_id.split('/')[1]}-fadel-full-arabic-diacritization\" # Choose your preferred name\n",
        "\n",
        "# QLoRA config\n",
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    \"bnb_4bit_compute_dtype\": torch.bfloat16, # Use float16 if bfloat16 is not supported\n",
        "    \"bnb_4bit_use_double_quant\": True,\n",
        "}\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank: Higher means more trainable parameters, adjust based on performance/memory\n",
        "    lora_alpha=64, # Scaling factor (often 2*r)\n",
        "    target_modules=[ # Modules to apply LoRA to - check model architecture if needed\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e50b48",
      "metadata": {
        "id": "20e50b48",
        "papermill": {
          "duration": 0.022288,
          "end_time": "2025-05-11T14:08:58.622216",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.599928",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b262599",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:58.716283Z",
          "iopub.status.busy": "2025-05-11T14:08:58.716007Z",
          "iopub.status.idle": "2025-05-11T14:08:58.880680Z",
          "shell.execute_reply": "2025-05-11T14:08:58.879869Z"
        },
        "id": "4b262599",
        "papermill": {
          "duration": 0.189758,
          "end_time": "2025-05-11T14:08:58.881968",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.692210",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"/kaggle/working/train_run-{new_model_id}\",\n",
        "    num_train_epochs=1, # Start with 1 epoch, increase if needed\n",
        "    per_device_train_batch_size=8, # Adjust based on GPU memory\n",
        "    gradient_accumulation_steps=2, # Increase effective batch size (4*4=16)\n",
        "    gradient_checkpointing=True, # Saves memory\n",
        "    optim=\"paged_adamw_32bit\", # Optimizer recommended for QLoRA\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500, # Save checkpoints periodically\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500, # Evaluate periodically\n",
        "    learning_rate=2e-4,\n",
        "    # weight_decay=0.001,\n",
        "    bf16=True if torch.cuda.is_bf16_supported() else False, # Use bf16 if available for faster training\n",
        "    # tf32=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\", # Or \"cosine\"\n",
        "    push_to_hub=True, # Set to True to push model adapters to Hub\n",
        "    report_to=\"wandb\", # Optional: set to \"tensorboard\" or \"none\"\n",
        "    load_best_model_at_end=True, # Load the best model based on eval loss\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"], # <--- ADD THIS LINE\n",
        "    run_name =f\"{new_model_id}\"\n",
        "    # Consider adding max_seq_length if sequences are very long, but tokenizer should handle it\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa88e075",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:58.929346Z",
          "iopub.status.busy": "2025-05-11T14:08:58.929123Z",
          "iopub.status.idle": "2025-05-11T14:08:58.934192Z",
          "shell.execute_reply": "2025-05-11T14:08:58.933650Z"
        },
        "id": "fa88e075",
        "outputId": "b961f61c-aba4-4a20-9db1-18116149df1c",
        "papermill": {
          "duration": 0.029384,
          "end_time": "2025-05-11T14:08:58.935128",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.905744",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "average_tokens_across_devices=False,\n",
              "batch_eval_metrics=False,\n",
              "bf16=True,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_persistent_workers=False,\n",
              "dataloader_pin_memory=True,\n",
              "dataloader_prefetch_factor=None,\n",
              "ddp_backend=None,\n",
              "ddp_broadcast_buffers=None,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_do_concat_batches=True,\n",
              "eval_on_start=False,\n",
              "eval_steps=500,\n",
              "eval_strategy=IntervalStrategy.STEPS,\n",
              "eval_use_gather_object=False,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=2,\n",
              "gradient_checkpointing=True,\n",
              "gradient_checkpointing_kwargs=None,\n",
              "greater_is_better=False,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_always_push=False,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=None,\n",
              "hub_strategy=HubStrategy.EVERY_SAVE,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_for_metrics=[],\n",
              "include_inputs_for_metrics=False,\n",
              "include_num_input_tokens_seen=False,\n",
              "include_tokens_per_second=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=['labels'],\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=0.0002,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=True,\n",
              "local_rank=0,\n",
              "log_level=passive,\n",
              "log_level_replica=warning,\n",
              "log_on_each_node=True,\n",
              "logging_dir=/kaggle/working/train_run-SmolLM2-135M-Instruct-fadel-full-arabic-diacritization/runs/May11_14-08-58_d5677477d90c,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=25,\n",
              "logging_strategy=IntervalStrategy.STEPS,\n",
              "lr_scheduler_kwargs={},\n",
              "lr_scheduler_type=SchedulerType.CONSTANT,\n",
              "max_grad_norm=0.3,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=eval_loss,\n",
              "mp_parameters=,\n",
              "neftune_noise_alpha=None,\n",
              "no_cuda=False,\n",
              "num_train_epochs=1,\n",
              "optim=OptimizerNames.PAGED_ADAMW,\n",
              "optim_args=None,\n",
              "optim_target_modules=None,\n",
              "output_dir=/kaggle/working/train_run-SmolLM2-135M-Instruct-fadel-full-arabic-diacritization,\n",
              "overwrite_output_dir=False,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=8,\n",
              "per_device_train_batch_size=8,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=True,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=['wandb'],\n",
              "restore_callback_states_from_checkpoint=False,\n",
              "resume_from_checkpoint=None,\n",
              "run_name=SmolLM2-135M-Instruct-fadel-full-arabic-diacritization,\n",
              "save_on_each_node=False,\n",
              "save_only_model=False,\n",
              "save_safetensors=True,\n",
              "save_steps=500,\n",
              "save_strategy=SaveStrategy.STEPS,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "skip_memory_metrics=True,\n",
              "tf32=None,\n",
              "torch_compile=False,\n",
              "torch_compile_backend=None,\n",
              "torch_compile_mode=None,\n",
              "torch_empty_cache_steps=None,\n",
              "torchdynamo=None,\n",
              "tp_size=0,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_cpu=False,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_liger_kernel=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.03,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.0,\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a5fee1",
      "metadata": {
        "id": "83a5fee1",
        "papermill": {
          "duration": 0.023528,
          "end_time": "2025-05-11T14:08:58.983037",
          "exception": false,
          "start_time": "2025-05-11T14:08:58.959509",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Prompt Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3435cb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:59.029510Z",
          "iopub.status.busy": "2025-05-11T14:08:59.029223Z",
          "iopub.status.idle": "2025-05-11T14:08:59.032729Z",
          "shell.execute_reply": "2025-05-11T14:08:59.032181Z"
        },
        "id": "8f3435cb",
        "papermill": {
          "duration": 0.028074,
          "end_time": "2025-05-11T14:08:59.033804",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.005730",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Constants for Prompt Formatting ---\n",
        "SYSTEM_PROMPT = \"<|im_start|>system\\nYou are an expert Arabic linguist. Your task is to add diacritics (Tashkeel) to the given Arabic text accurately.<|im_end|>\"\n",
        "USER_PROMPT_TEMPLATE = \"<|im_start|>user\\nDiacritize the following text:\\n{undiacritized_text}<|im_end|>\"\n",
        "ASSISTANT_PROMPT_TEMPLATE = \"<|im_start|>assistant\\n{diacritized_text}<|im_end|>\" # Note: includes EOS token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf32c47",
      "metadata": {
        "id": "4bf32c47",
        "papermill": {
          "duration": 0.022783,
          "end_time": "2025-05-11T14:08:59.079291",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.056508",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0effc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:59.126309Z",
          "iopub.status.busy": "2025-05-11T14:08:59.125671Z",
          "iopub.status.idle": "2025-05-11T14:08:59.280667Z",
          "shell.execute_reply": "2025-05-11T14:08:59.279859Z"
        },
        "id": "6d0effc0",
        "outputId": "09473163-0636-48b1-a1da-82b1d0c86fc0",
        "papermill": {
          "duration": 0.179985,
          "end_time": "2025-05-11T14:08:59.282042",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.102057",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__notebook__.ipynb\r\n"
          ]
        }
      ],
      "source": [
        "!ls '/kaggle/working/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cef480c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:59.331188Z",
          "iopub.status.busy": "2025-05-11T14:08:59.330532Z",
          "iopub.status.idle": "2025-05-11T14:08:59.717083Z",
          "shell.execute_reply": "2025-05-11T14:08:59.716375Z"
        },
        "id": "1cef480c",
        "papermill": {
          "duration": 0.412439,
          "end_time": "2025-05-11T14:08:59.718591",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.306152",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# --- Data Loading ---\n",
        "import datasets\n",
        "# dataset = load_dataset(dataset_id) # Load the training split\n",
        "# dataset = load_dataset(dataset_id) # Load the training split\n",
        "!cp -r '/kaggle/input/fadel-diacritization/fadel-diacritization-local' ./data\n",
        "dataset = datasets.load_from_disk('./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac4e431",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:59.766547Z",
          "iopub.status.busy": "2025-05-11T14:08:59.766035Z",
          "iopub.status.idle": "2025-05-11T14:08:59.771479Z",
          "shell.execute_reply": "2025-05-11T14:08:59.770737Z"
        },
        "id": "eac4e431",
        "outputId": "761abb6e-755b-41b2-b070-a4240a1a7a01",
        "papermill": {
          "duration": 0.030112,
          "end_time": "2025-05-11T14:08:59.772644",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.742532",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 128334\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 6767\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 6454\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83454ecd",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "92760f395fae420fbb02a1c13fb858b3",
            "7088c5981eb742d0839dd642b13d4456",
            "3bc0763a6c9a4f8bb6c88edf565a0a61",
            "0062e693da134324ae10a97e5df11b85",
            "e8fa4369198e4142a1acca3db881230d",
            "682e19180f634d60bd2abbeefaa4d208",
            "e9dc2739330f40c08627d97331fd3604",
            "c35fffcdfedd4bf3b500db97bff10904",
            "40f3ccd802114b65b2669a5e31113de3",
            "aa4ec1b13b684dc29740a291515adfee",
            "e8b3608972004de48dc3a883a7ba467a",
            "173c214d7cd94d8d99a8340ff68a9d27",
            "8e054e0e2dae400090ffa6e15d544436",
            "fee96650221646609bcda401924edb21",
            "c2e875bca1dd44d59dc5c6684b450ee2",
            "b256bef7ed5748c0841c1447c25a364a",
            "acf550b4cc3d49e6856d065a50fab2f1"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-05-11T14:08:59.819880Z",
          "iopub.status.busy": "2025-05-11T14:08:59.819678Z",
          "iopub.status.idle": "2025-05-11T14:14:22.853711Z",
          "shell.execute_reply": "2025-05-11T14:14:22.852740Z"
        },
        "id": "83454ecd",
        "outputId": "cc1c8158-b26a-4208-bee9-ad9f823352e3",
        "papermill": {
          "duration": 323.058843,
          "end_time": "2025-05-11T14:14:22.854843",
          "exception": false,
          "start_time": "2025-05-11T14:08:59.796000",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92760f395fae420fbb02a1c13fb858b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7088c5981eb742d0839dd642b13d4456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bc0763a6c9a4f8bb6c88edf565a0a61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0062e693da134324ae10a97e5df11b85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8fa4369198e4142a1acca3db881230d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatting prompts...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "682e19180f634d60bd2abbeefaa4d208",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/128334 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9dc2739330f40c08627d97331fd3604",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6767 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c35fffcdfedd4bf3b500db97bff10904",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40f3ccd802114b65b2669a5e31113de3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/128334 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa4ec1b13b684dc29740a291515adfee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6767 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8b3608972004de48dc3a883a7ba467a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/6454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "173c214d7cd94d8d99a8340ff68a9d27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/128334 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e054e0e2dae400090ffa6e15d544436",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6767 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee96650221646609bcda401924edb21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating labels...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2e875bca1dd44d59dc5c6684b450ee2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/128334 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b256bef7ed5748c0841c1447c25a364a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6767 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acf550b4cc3d49e6856d065a50fab2f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6454 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 128334\n",
            "Eval dataset size: 6454\n",
            "Example processed sample:\n",
            "{'input_ids': [1, 9690, 198, 2683, 359, 354, 4507, 12428, 8229, 363, 30, 2789, 3856, 314, 288, 803, 801, 387, 608, 747, 365, 68, 1317, 396, 299, 25, 288, 260, 1836, 12428, 1694, 8589, 30, 2, 198, 1, 4093, 198, 12117, 387, 608, 836, 260, 1695, 1694, 42, 198, 23939, 12667, 23939, 14787, 122, 21706, 34966, 14787, 121, 21706, 45704, 12667, 21706, 14787, 120, 20602, 48934, 14787, 126, 48934, 18223, 36395, 18223, 27819, 163, 113, 23939, 12667, 164, 227, 45726, 163, 132, 12667, 27438, 10805, 35964, 23339, 34966, 23339, 27749, 22682, 21706, 10805, 14787, 122, 10805, 21706, 34966, 10805, 14787, 230, 2, 198, 1, 520, 9531, 198, 23939, 14776, 12667, 14776, 23939, 29372, 14787, 122, 14776, 21706, 14776, 34966, 14776, 14787, 121, 30203, 21706, 38105, 14776, 45704, 14776, 12667, 25694, 21706, 14776, 14787, 120, 14776, 20602, 29372, 48934, 14776, 14787, 126, 30203, 48934, 29372, 18223, 164, 231, 36395, 25694, 18223, 29372, 27819, 29372, 163, 113, 30203, 23939, 12667, 14776, 164, 227, 45726, 14776, 163, 132, 14776, 12667, 14776, 27438, 14776, 10805, 35964, 14776, 23339, 30203, 34966, 25694, 23339, 27749, 30203, 22682, 30203, 21706, 14776, 10805, 14787, 122, 14776, 10805, 21706, 25694, 34966, 164, 229, 10805, 14787, 230, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 23939, 14776, 12667, 14776, 23939, 29372, 14787, 122, 14776, 21706, 14776, 34966, 14776, 14787, 121, 30203, 21706, 38105, 14776, 45704, 14776, 12667, 25694, 21706, 14776, 14787, 120, 14776, 20602, 29372, 48934, 14776, 14787, 126, 30203, 48934, 29372, 18223, 164, 231, 36395, 25694, 18223, 29372, 27819, 29372, 163, 113, 30203, 23939, 12667, 14776, 164, 227, 45726, 14776, 163, 132, 14776, 12667, 14776, 27438, 14776, 10805, 35964, 14776, 23339, 30203, 34966, 25694, 23339, 27749, 30203, 22682, 30203, 21706, 14776, 10805, 14787, 122, 14776, 10805, 21706, 25694, 34966, 164, 229, 10805, 14787, 230, 2]}\n",
            "\n",
            "Decoded Example:\n",
            "<|im_start|>system\n",
            "You are an expert Arabic linguist. Your task is to add diacritics (Tashkeel) to the given Arabic text accurately.<|im_end|>\n",
            "<|im_start|>user\n",
            "Diacritize the following text:\n",
            "ولو جمع ثم علم ترك ركن من الأولى بطلتا ويعيدهما جامعا ،<|im_end|>\n",
            "<|im_start|>assistant\n",
            "وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ،<|im_end|>\n",
            "\n",
            "Labels (first 100 tokens):\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
          ]
        }
      ],
      "source": [
        "# --- Tokenizer Loading ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"right\", # Pad right for Causal LM\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False # Qwen tokenizers might need use_fast=False\n",
        ")\n",
        "# Set pad token if not already set (Qwen uses <|endoftext|>)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Set pad_token to eos_token:\", tokenizer.pad_token)\n",
        "\n",
        "# --- Preprocessing Functions ---\n",
        "def remove_diacritics(text):\n",
        "    \"\"\"Removes Arabic diacritics using pyarabic.\"\"\"\n",
        "    return pyarabic.araby.strip_tashkeel(text)\n",
        "\n",
        "def format_prompt(example):\n",
        "    \"\"\"Creates the full instruction prompt for a given example.\"\"\"\n",
        "    diacritized = example['text']\n",
        "    # Basic cleaning: Remove extra spaces, ensure consistency\n",
        "    diacritized = re.sub(r'\\s+', ' ', diacritized).strip()\n",
        "    undiacritized = remove_diacritics(diacritized)\n",
        "\n",
        "    if not undiacritized or not diacritized: # Skip empty examples if any\n",
        "        return None\n",
        "\n",
        "    user_prompt = USER_PROMPT_TEMPLATE.format(undiacritized_text=undiacritized)\n",
        "    assistant_prompt = ASSISTANT_PROMPT_TEMPLATE.format(diacritized_text=diacritized)\n",
        "\n",
        "    full_prompt = f\"{SYSTEM_PROMPT}\\n{user_prompt}\\n{assistant_prompt}\"\n",
        "    return {\"text\": full_prompt, \"input_text\": user_prompt, \"output_text\": assistant_prompt}\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenizes the formatted prompts.\"\"\"\n",
        "    # Note: Qwen uses specific chat template logic, but here we format manually\n",
        "    # We tokenize the full prompt. The loss will be calculated only on the assistant's part.\n",
        "    tokenized_outputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False, # Trainer will handle padding with data collator\n",
        "        max_length=512, # Adjust based on dataset analysis and memory\n",
        "        add_special_tokens=False # We added special tokens manually in format_prompt\n",
        "    )\n",
        "    return tokenized_outputs\n",
        "\n",
        "def create_labels(examples):\n",
        "    \"\"\"Creates labels for Causal LM training. Masks input tokens.\"\"\"\n",
        "    labels = []\n",
        "    tokenizer_eos_token_id = tokenizer.eos_token_id # Store for quicker access\n",
        "\n",
        "    for i in range(len(examples['input_ids'])):\n",
        "        # Tokenize the input part (user prompt) and output part (assistant response) separately\n",
        "        # to find where the assistant's response starts.\n",
        "        # This requires knowing the exact format including special tokens.\n",
        "        # Let's re-tokenize the input and output parts used to build the full prompt.\n",
        "        # This is slightly inefficient but ensures accuracy.\n",
        "\n",
        "        # Reconstruct the input section for tokenization\n",
        "        prompt_input_part = f\"{SYSTEM_PROMPT}\\n{examples['input_text'][i]}\\n<|im_start|>assistant\\n\"\n",
        "        input_tokens = tokenizer(\n",
        "            prompt_input_part,\n",
        "            truncation=False, # Don't truncate here, we just need the length\n",
        "            padding=False,\n",
        "            add_special_tokens=False\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "        input_token_len = len(input_tokens)\n",
        "        full_tokenized_len = len(examples['input_ids'][i])\n",
        "\n",
        "        # Initialize labels with -100 (ignored by loss function)\n",
        "        label = [-100] * full_tokenized_len\n",
        "\n",
        "        # Unmask the tokens corresponding to the assistant's response\n",
        "        # Start unmasking *after* the input part tokens\n",
        "        for j in range(input_token_len, full_tokenized_len):\n",
        "             # Check if it's not a pad token (though DataCollator handles this too)\n",
        "            if examples['input_ids'][i][j] != tokenizer.pad_token_id:\n",
        "                 label[j] = examples['input_ids'][i][j]\n",
        "\n",
        "        # Ensure the EOS token at the very end is included in labels if present\n",
        "        if examples['input_ids'][i][-1] == tokenizer_eos_token_id:\n",
        "             if label[-1] == -100 : # Make sure it wasn't masked out by error\n",
        "                 label[-1] = tokenizer_eos_token_id\n",
        "        else:\n",
        "             # If the full sequence was truncated before EOS, we might not have it.\n",
        "             pass # Or add EOS manually if required by model/training setup\n",
        "\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return {\"labels\": labels}\n",
        "\n",
        "\n",
        "# --- Apply Preprocessing ---\n",
        "print(\"Formatting prompts...\")\n",
        "formatted_dataset = dataset.map(format_prompt, remove_columns=['text'])\n",
        "# Filter out None examples if any were created\n",
        "formatted_dataset = formatted_dataset.filter(lambda example: example['text'] is not None)\n",
        "\n",
        "print(\"Tokenizing data...\")\n",
        "# Keep original input/output text temporarily for label creation\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text'] # Remove original combined text after tokenization\n",
        ")\n",
        "\n",
        "print(\"Creating labels...\")\n",
        "final_dataset = tokenized_dataset.map(\n",
        "    create_labels,\n",
        "    batched=True,\n",
        "    remove_columns=['input_text', 'output_text'] # Remove helper columns\n",
        ")\n",
        "\n",
        "# Shuffle the datasets\n",
        "train_dataset = final_dataset[\"train\"]\n",
        "eval_dataset = final_dataset[\"validation\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
        "print(\"Example processed sample:\")\n",
        "print(train_dataset[0])\n",
        "print(\"\\nDecoded Example:\")\n",
        "print(tokenizer.decode(train_dataset[0]['input_ids']))\n",
        "print(\"\\nLabels (first 100 tokens):\")\n",
        "print(train_dataset[0]['labels'][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d4b2a7e",
      "metadata": {
        "id": "1d4b2a7e",
        "papermill": {
          "duration": 0.024297,
          "end_time": "2025-05-11T14:14:22.904310",
          "exception": false,
          "start_time": "2025-05-11T14:14:22.880013",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e35d90",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "66a390add6b54b9caa859f10ac1a186b",
            "5afcdb4a78624b9fbebe99135309cf69",
            "a15ea74b64c940549ec28078409f9636"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-05-11T14:14:22.954658Z",
          "iopub.status.busy": "2025-05-11T14:14:22.954064Z",
          "iopub.status.idle": "2025-05-11T14:14:38.028169Z",
          "shell.execute_reply": "2025-05-11T14:14:38.027413Z"
        },
        "id": "10e35d90",
        "outputId": "b0b13ba8-1c33-4db0-bcc8-5b8096842bf6",
        "papermill": {
          "duration": 15.100689,
          "end_time": "2025-05-11T14:14:38.029330",
          "exception": false,
          "start_time": "2025-05-11T14:14:22.928641",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66a390add6b54b9caa859f10ac1a186b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5afcdb4a78624b9fbebe99135309cf69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a15ea74b64c940549ec28078409f9636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model loaded.\n",
            "Model prepared for k-bit training.\n",
            "PEFT model created.\n",
            "trainable params: 9,768,960 || all params: 144,283,968 || trainable%: 6.7706\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Load Base Model with Quantization ---\n",
        "print(\"Loading base model...\")\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=bnb_config[\"load_in_4bit\"],\n",
        "    bnb_4bit_quant_type=bnb_config[\"bnb_4bit_quant_type\"],\n",
        "    bnb_4bit_compute_dtype=bnb_config[\"bnb_4bit_compute_dtype\"],\n",
        "    bnb_4bit_use_double_quant=bnb_config[\"bnb_4bit_use_double_quant\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\", # Automatically distribute across available GPUs\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- Prepare Model for PEFT ---\n",
        "# Enable gradient checkpointing for memory saving\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare model for k-bit training (additional adjustments for quantization)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"Model prepared for k-bit training.\")\n",
        "\n",
        "# --- Apply LoRA ---\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"PEFT model created.\")\n",
        "\n",
        "# --- Print Trainable Parameters ---\n",
        "model.print_trainable_parameters()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfc17e6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:14:38.080458Z",
          "iopub.status.busy": "2025-05-11T14:14:38.080198Z",
          "iopub.status.idle": "2025-05-11T14:14:38.084307Z",
          "shell.execute_reply": "2025-05-11T14:14:38.083599Z"
        },
        "id": "adfc17e6",
        "outputId": "4cc0648c-1bf5-4e90-ab61-4226edf9cc77",
        "papermill": {
          "duration": 0.030493,
          "end_time": "2025-05-11T14:14:38.085267",
          "exception": false,
          "start_time": "2025-05-11T14:14:38.054774",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using DataCollatorForSeq2Seq.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq # Make sure this import is added\n",
        "\n",
        "# --- Data Collator ---\n",
        "# Use DataCollatorForSeq2Seq which robustly handles padding for inputs and labels\n",
        "print(\"Using DataCollatorForSeq2Seq.\")\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=True, # Ensure padding is enabled\n",
        "    label_pad_token_id=-100 # Explicitly define the padding value for labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8ee0d5",
      "metadata": {
        "id": "fc8ee0d5",
        "papermill": {
          "duration": 0.02445,
          "end_time": "2025-05-11T14:14:38.134465",
          "exception": false,
          "start_time": "2025-05-11T14:14:38.110015",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Train and push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e249ee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:14:38.185063Z",
          "iopub.status.busy": "2025-05-11T14:14:38.184539Z",
          "iopub.status.idle": "2025-05-11T14:14:39.283252Z",
          "shell.execute_reply": "2025-05-11T14:14:39.282709Z"
        },
        "id": "89e249ee",
        "papermill": {
          "duration": 1.125686,
          "end_time": "2025-05-11T14:14:39.284533",
          "exception": false,
          "start_time": "2025-05-11T14:14:38.158847",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae93c101",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:14:39.336523Z",
          "iopub.status.busy": "2025-05-11T14:14:39.335954Z",
          "iopub.status.idle": "2025-05-11T14:14:39.340637Z",
          "shell.execute_reply": "2025-05-11T14:14:39.340011Z"
        },
        "id": "ae93c101",
        "outputId": "13643b57-5854-4e32-bf6a-2b95411c4dc0",
        "papermill": {
          "duration": 0.031589,
          "end_time": "2025-05-11T14:14:39.341694",
          "exception": false,
          "start_time": "2025-05-11T14:14:39.310105",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args.push_to_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597b2fce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T14:14:39.392254Z",
          "iopub.status.busy": "2025-05-11T14:14:39.391782Z",
          "iopub.status.idle": "2025-05-11T20:54:35.840207Z",
          "shell.execute_reply": "2025-05-11T20:54:35.839279Z"
        },
        "id": "597b2fce",
        "outputId": "bd8a4bf4-e938-4d6b-baaa-870f6092a796",
        "papermill": {
          "duration": 23996.475389,
          "end_time": "2025-05-11T20:54:35.841828",
          "exception": false,
          "start_time": "2025-05-11T14:14:39.366439",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250511_141440-vjcqv0hx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mSmolLM2-135M-Instruct-fadel-full-arabic-diacritization\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/bishertello-/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/bishertello-/huggingface/runs/vjcqv0hx\u001b[0m\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8021' max='8021' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8021/8021 6:39:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.134000</td>\n",
              "      <td>0.132819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.099900</td>\n",
              "      <td>0.102239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.081000</td>\n",
              "      <td>0.085592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.069100</td>\n",
              "      <td>0.076976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.066800</td>\n",
              "      <td>0.069491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.064200</td>\n",
              "      <td>0.065941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>0.063670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.058600</td>\n",
              "      <td>0.058567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.053900</td>\n",
              "      <td>0.055704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>0.054056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.051700</td>\n",
              "      <td>0.052810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.050300</td>\n",
              "      <td>0.052167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.046700</td>\n",
              "      <td>0.049573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.047400</td>\n",
              "      <td>0.048778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.043400</td>\n",
              "      <td>0.048833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.047300</td>\n",
              "      <td>0.047031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Saving LoRA adapter...\n",
            "Adapter saved to /kaggle/working/train_run-SmolLM2-135M-Instruct-fadel-full-arabic-diacritization/final_adapter\n",
            "Adapter pushed to Hugging Face Hub.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Optional: Login to WandB\n",
        "# wandb.login()\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "\n",
        "    # You could add compute_metrics=compute_metrics here for custom eval\n",
        ")\n",
        "\n",
        "# --- Start Training ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- Save the Final LoRA Adapter ---\n",
        "print(\"Saving LoRA adapter...\")\n",
        "final_adapter_dir = os.path.join(training_args.output_dir, \"final_adapter\")\n",
        "trainer.model.save_pretrained(final_adapter_dir)\n",
        "tokenizer.save_pretrained(final_adapter_dir) # Save tokenizer with adapter\n",
        "print(f\"Adapter saved to {final_adapter_dir}\")\n",
        "\n",
        "trainer.push_to_hub()\n",
        "print(\"Adapter pushed to Hugging Face Hub.\") #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ac766a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T20:54:35.905366Z",
          "iopub.status.busy": "2025-05-11T20:54:35.905127Z",
          "iopub.status.idle": "2025-05-11T20:54:37.799807Z",
          "shell.execute_reply": "2025-05-11T20:54:37.798714Z"
        },
        "id": "b8ac766a",
        "outputId": "32f088b5-7aa1-4381-9fbc-d85bc0020f07",
        "papermill": {
          "duration": 1.925949,
          "end_time": "2025-05-11T20:54:37.801267",
          "exception": false,
          "start_time": "2025-05-11T20:54:35.875318",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: wandb\r\n",
            "Version: 0.19.6\r\n",
            "Summary: A CLI and library for interacting with the Weights & Biases API.\r\n",
            "Home-page: \r\n",
            "Author: \r\n",
            "Author-email: Weights & Biases <support@wandb.com>\r\n",
            "License: MIT License\r\n",
            "\r\n",
            "Copyright (c) 2021 Weights and Biases, Inc.\r\n",
            "\r\n",
            "Permission is hereby granted, free of charge, to any person obtaining a copy\r\n",
            "of this software and associated documentation files (the \"Software\"), to deal\r\n",
            "in the Software without restriction, including without limitation the rights\r\n",
            "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n",
            "copies of the Software, and to permit persons to whom the Software is\r\n",
            "furnished to do so, subject to the following conditions:\r\n",
            "\r\n",
            "The above copyright notice and this permission notice shall be included in all\r\n",
            "copies or substantial portions of the Software.\r\n",
            "\r\n",
            "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n",
            "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n",
            "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n",
            "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n",
            "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n",
            "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n",
            "SOFTWARE.\r\n",
            "Location: /usr/local/lib/python3.11/dist-packages\r\n",
            "Requires: click, docker-pycreds, gitpython, platformdirs, protobuf, psutil, pydantic, pyyaml, requests, sentry-sdk, setproctitle, setuptools, typing-extensions\r\n",
            "Required-by: \r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30841a5d",
      "metadata": {
        "id": "30841a5d",
        "papermill": {
          "duration": 0.028239,
          "end_time": "2025-05-11T20:54:37.858301",
          "exception": false,
          "start_time": "2025-05-11T20:54:37.830062",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "be448617",
      "metadata": {
        "id": "be448617",
        "papermill": {
          "duration": 0.026677,
          "end_time": "2025-05-11T20:54:37.912092",
          "exception": false,
          "start_time": "2025-05-11T20:54:37.885415",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e36ac05",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-11T20:54:37.966974Z",
          "iopub.status.busy": "2025-05-11T20:54:37.966682Z",
          "iopub.status.idle": "2025-05-11T20:54:54.160713Z",
          "shell.execute_reply": "2025-05-11T20:54:54.159796Z"
        },
        "id": "9e36ac05",
        "outputId": "89eb2fa1-59d0-4a16-85a3-190398b2c23f",
        "papermill": {
          "duration": 16.223317,
          "end_time": "2025-05-11T20:54:54.161933",
          "exception": false,
          "start_time": "2025-05-11T20:54:37.938616",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for inference...\n",
            "Model ready for inference.\n",
            "\n",
            "Input Text: اكل الولد التفاحة في الحديقة\n",
            "Model Output: اكْلَ الْوَلَدِ التُّفَاحَةَ فِي الْحَدِيقَةِ\n",
            "\n",
            "Input Text: بسم الله الرحمن الرحيم\n",
            "Model Output: بِسْمِ اللَّهِ الرَّحْمَنِ الرَّحِيمِ\n",
            "\n",
            "Input Text: تعتبر اللغة العربية من أصعب اللغات في العالم\n",
            "Model Output: تُعْتَبَرُ اللُّغَةُ الْعَرَبِيَّةُ مِنْ أَصْعَبِ اللُّغَاتِ فِي الْعَالِمِ\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "import re\n",
        "\n",
        "# --- Load Base Model and Adapter for Inference ---\n",
        "print(\"Loading model for inference...\")\n",
        "\n",
        "# Load the base model with quantization (same config as training)\n",
        "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=quantization_config, # Use the same BNB config\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load the PEFT adapter\n",
        "adapter_path = final_adapter_dir # Use the path where the adapter was saved\n",
        "inference_model = PeftModel.from_pretrained(base_model_for_inference, adapter_path)\n",
        "inference_model.eval() # Set to evaluation mode\n",
        "\n",
        "print(\"Model ready for inference.\")\n",
        "\n",
        "# --- Inference Function ---\n",
        "def diacritize_text(text, model, tokenizer):\n",
        "    \"\"\"Takes undiacritized text and returns the model's diacritized version.\"\"\"\n",
        "    # Prepare the prompt\n",
        "    undiacritized = re.sub(r'\\s+', ' ', text).strip() # Basic cleaning\n",
        "    user_prompt = USER_PROMPT_TEMPLATE.format(undiacritized_text=undiacritized)\n",
        "    # Construct the prompt that the model expects *up to* the assistant's turn\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\n{user_prompt}\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "\n",
        "    # Generate output\n",
        "    # Adjust max_new_tokens based on expected output length; add other generation params as needed\n",
        "    # Using greedy decoding here, consider beam search, top-k, top-p for potentially better results\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=len(inputs['input_ids'][0]) + 100, # Allow for diacritics and some buffer\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        do_sample=False # Use greedy decoding for consistency\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    # Decode only the *newly generated* tokens\n",
        "    generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
        "    decoded_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Sometimes the model might hallucinate extra text after the main diacritization.\n",
        "    # A simple heuristic: stop at the first newline or excessive length if needed.\n",
        "    # decoded_output = decoded_output.split('\\n')[0] # Simple cleanup if needed\n",
        "\n",
        "    return decoded_output\n",
        "\n",
        "# --- Example Usage ---\n",
        "input_text = \"اكل الولد التفاحة في الحديقة\"\n",
        "print(f\"\\nInput Text: {input_text}\")\n",
        "\n",
        "diacritized_output = diacritize_text(input_text, inference_model, tokenizer)\n",
        "print(f\"Model Output: {diacritized_output}\")\n",
        "\n",
        "input_text_2 = \"بسم الله الرحمن الرحيم\"\n",
        "print(f\"\\nInput Text: {input_text_2}\")\n",
        "diacritized_output_2 = diacritize_text(input_text_2, inference_model, tokenizer)\n",
        "print(f\"Model Output: {diacritized_output_2}\")\n",
        "\n",
        "input_text_3 = \"تعتبر اللغة العربية من أصعب اللغات في العالم\"\n",
        "print(f\"\\nInput Text: {input_text_3}\")\n",
        "diacritized_output_3 = diacritize_text(input_text_3, inference_model, tokenizer)\n",
        "print(f\"Model Output: {diacritized_output_3}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7152826,
          "sourceId": 11421184,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 24471.9566,
      "end_time": "2025-05-11T20:54:57.035470",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-11T14:07:05.078870",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
